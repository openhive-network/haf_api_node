# The name of the ZFS storage pool for HAF to use
ZPOOL="/workspace/hafbe/haf-pool"
# The name of the dataset on $ZPOOL where HAF will store its data
# HAF won't read/write anything outside of $ZPOOL/$TOP_LEVEL_DATASET,
# so you can have, e.g., multiple HAF installations on the same
# pool by changing TOP_LEVEL_DATASET
TOP_LEVEL_DATASET="haf-datadir"

# these defaults usually don't need changing
ZPOOL_MOUNT_POINT="/${ZPOOL}"
TOP_LEVEL_DATASET_MOUNTPOINT="${ZPOOL_MOUNT_POINT}/${TOP_LEVEL_DATASET}"

# COMPOSE_PROFILES are the list of HAF services you want to control when
# you run `docker compose up` etc.  It's a comma-separated list of profiles
# taken from:
# - core: the minimal HAF system of a database and hived
# - admin: useful tools for administrating HAF: pgadmin, pghero
# - apps: core HAF apps: hivemind, hafah, hafbe (balance-tracker is a subapp)
# - servers: services for routing/caching API calls: haproxy, jussi (JSON caching), varnish (REST caching)
# - monitoring: services for Prometheus, Grafana, Loki, Cadvisor , Nodeexporter, Promtail, Postresexporter, Blackboxexporter...
# COMPOSE_PROFILES="core,admin,hafah,hivemind,servers"
# COMPOSE_PROFILES="core,admin,hafah,hafbe,hivemind,servers,monitoring"
COMPOSE_PROFILES="core,admin,hafah,hafbe,servers"

# The registry where Hive docker images are pulled from.  Normally, you
# should set this to the default, `registry.hive.blog` or Docker Hub,
# where stable images will be published.  If you want to run pre-release
# images, change this to `registry.gitlab.syncad.com/hive` where both CI
# builds are automatically pushed.
# HIVE_API_NODE_REGISTRY=registry.hive.blog
HIVE_API_NODE_REGISTRY=registry.gitlab.syncad.com/hive
# HIVE_API_NODE_REGISTRY=hiveio

# To use the same tagged version of all the Hive API node images,
# set it here.  You can override the tags for individual images
# below
HIVE_API_NODE_VERSION=1.27.6rc1

# Global settings

# override the HAF core image's version and registry image here:
HAF_REGISTRY=${HIVE_API_NODE_REGISTRY}/haf/base_instance
# HAF_VERSION=3a7456f2
# HAF_VERSION=236db88f

HAF_DATA_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}"
HAF_LOG_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/logs"
HAF_WAL_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/shared_memory/haf_wal"
# If you need to massive sync HAF (i.e. you are not using a ZFS snapshot),
# then you can sync faster by temporarily using an in-memory shared_memory.bin.
# To do this, comment out the line below and uncomment the one after, and
# mount an appropriate tmpfs filesystem there.
# After the sync has finished, do `docker compose down` then move the shared_memory.bin
# file to the shared_memory directory, edit this file to restore original values, and
# `docker compose up -d` to restart HAF.
# HAF_SHM_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/shared_memory"
# HAF_SHM_DIRECTORY="/mnt/haf_shared_mem"
HAF_SHM_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/shared_memory"

# The docker compose project name, gets prefixed onto each container name
PROJECT_NAME=haf-world
# The docker network name, if you run two HAF instances on the same server,
# give them different network names to keep them isolated.  Otherwise
# unimportant.
NETWORK_NAME=haf

# List of arguments for the HAF service
#ARGUMENTS="--dump-snapshot=20230821"
#ARGUMENTS="--skip-hived"
#ARGUMENTS="--replay-blockchain"
ARGUMENTS="--replay-blockchain --stop-replay-at-block 5000000"
#
# Example how to use monitoring services
#
# ARGUMENTS="--replay-blockchain --stop-replay-at-block 5000000 --exit-before-sync --block-stats-report-output=NOTIFY --block-stats-report-type=FULL --notifications-endpoint=hived-pme:9185"
#
# Mandatory options are:
# --block-stats-report-output=NOTIFY --block-stats-report-type=FULL --notifications-endpoint=hived-pme:9185
# Which activate endpoint notification for hived-pme service (log converter from hived to Prometheus metrics)
#
#  By default, 5 dashboards are available:
#
# - Blockstats(available after replay phase, showing live state of block, times, delays etc)
# - cAdvisor Docker Container - status of containers in stack, CPU, Memory, I/O, Network...)
# - Node Exporter Full - full state of host on which haf-api-node is running(full overview of available metrics)
# - Monitor Services - status of containers included in the monitoring system
# - PostgreSQL Databases - databases stats from postgresexporter
#
# Additional logs are collected from all containers in the stack via Loki and Promtail
# Default login and password for Grafana is admin/admin - remember to change it after first login
# Statistics provided by Grafana are available at the host address on port 3000 (http(s)://hostname:3000)
#

# The default setup will run the recommended version of HAfAH,
# you can run a custom version by un-commenting and modifying the
# values below
HAFAH_REGISTRY=${HIVE_API_NODE_REGISTRY}/hafah/setup
# HAFAH_VERSION=9556cd0f
# HAFAH_VERSION=a85fc0b4

# The default setup will run the recommended version of Hivemind using the values
# below.  You can override them here to run a custom version of Hivemind
HIVEMIND_INSTANCE_IMAGE=${HIVE_API_NODE_REGISTRY}/hivemind/instance
# HIVEMIND_INSTANCE_VERSION=f09bd298


# The default setup will run the recommended version of balance tracker,
# you can run a custom version by un-commenting and modifying the
# values below
BALANCE_TRACKER_REGISTRY=${HIVE_API_NODE_REGISTRY}/balance_tracker
# BALANCE_TRACKER_VERSION=66452803
# BALANCE_TRACKER_VERSION=c9906c48


# REPUTATION_TRACKER_ADDON
HAF_REPUTATION_TRACKER_REGISTRY=${HIVE_API_NODE_REGISTRY}/reputation_tracker
#HAF_REPUTATION_TRACKER_VERSION=f9f74604

HAF_VERSION=d70a9632

HAFAH_VERSION=e977d9e8
BALANCE_TRACKER_VERSION=b51cb031
HAF_BLOCK_EXPLORER_VERSION=9e5a1bf2
REPUTATION_TRACKER_VERSION=6dfbe231

HIVEMIND_INSTANCE_VERSION=5aa95785

# REPTRACKER_SCHEMA="hafbe-rt"
# BTRACKER_SCHEMA="hafbe-bt"
# The default setup will run the recommended version of HAF block explorer,
# you can run a custom version by un-commenting and modifying the
# values below
HAF_BLOCK_EXPLORER_REGISTRY=${HIVE_API_NODE_REGISTRY}/haf_block_explorer
# HAF_BLOCK_EXPLORER_VERSION=b268faed
# HAF_BLOCK_EXPLORER_VERSION=354960a1


# The default setup uses "Jussi" as the API reverse proxy & cache for the old JSON-RPC-style
# calls.  There is an alternate reverse proxy, "Drone", that you can choose to use instead:
# https://hive.blog/hive-139531/@deathwing/announcing-drone-or-leveling-up-hive-api-nodes-and-user-experience
# To replace Jussi with Drone, uncomment the next line:
# JSONRPC_API_SERVER_NAME=drone

# The default setup will run the recommended version of Jussi
# you can run a custom version by un-commenting and modifying the
# values below
# JUSSI_REGISTRY=${HIVE_API_NODE_REGISTRY}/jussi
# JUSSI_VERSION=latest
# JUSSI_REDIS_MAX_MEMORY=8G

# If you have chosen to run Drone instead of Jussi, it will run the
# this version by default.  You can run a custom version by un-commenting
# and modifying the values below
# DRONE_REGISTRY=${HIVE_API_NODE_REGISTRY}/drone
# DRONE_VERSION=latest

# In the default configuration, synchronous broadcast_transaction calls are not handled by
# your local stack, but instead are sent to a dedicated hived instance on api.hive.blog.
# (asynchronous broadcast_transaction calls and all other hived calls are always handled by
# your local instance of hived).
# Synchronous calls can easily tie up your hived node and cause performance problems for
# all hived API calls.  For that reason, synchronous broadcast calls are deprecated.  On
# public API servers, we typically run a separate hived instance for synchronous calls,
# so if they cause performance problems, it only impacts other users making synchronous
# calls.
# To avoid forcing every haf_api_node operator to run a second hived server, the default
# config forwards these disruptive calls to a public server dedicated to the purpose.
# If you want to handle these calls using your local hived node, or you want to forward
# these calls to a different server, override these variables:
# the values below will cause synchronous broadcasts to be handled by your own hived
# SYNC_BROADCAST_BACKEND_SERVER=haf
# SYNC_BROADCAST_BACKEND_PORT=8091
# SYNC_BROADCAST_BACKEND_SSL=no-ssl

# For running a full stack:
# if you need to run a custom image (for example, to use ACME DNS challenges), specify it here
# CADDY_IMAGE=${HIVE_API_NODE_REGISTRY}/haf_api_node/caddy
# CADDY_VERSION=2.7.4-alpine-with-cloudflare

# The hostname you'll be running this server on.  There are several ways you can configure
# this.  Some examples:
# - to serve API using HTTPS, with automatic redirect from HTTP -> HTTPS, just give the
#   hostname:
#     PUBLIC_HOSTNAME="your.hostname.com"
# - to serve using only HTTP (if you have nginx or something else handling SSL termination),
#   you can use:
#     PUBLIC_HOSTNAME="http://your.hostname.com"
#   or even:
#     PUBLIC_HOSTNAME="http://"
#   if you want to respond on any hostname
# - to serve on either HTTP or HTTPS (i.e., respond to HTTP requests in the clear, instead of
#   issuing a redirect):
#     PUBLIC_HOSTNAME="http://your.hostname.com, https://your.hostname.com"
# - to serve on multiple hostnames, separate them with a comma and space:
#     PUBLIC_HOSTNAME="your.hostname.com, your.other-hostname.net"
PUBLIC_HOSTNAME="your.hostname.com"

# By default, we're configured to use a self-signed SSL certificate (by including the
# file below, which tells Caddy to generate a self-signed certificate).  To obtain a real
# certificate from LetsEncrypt or otherwise, you can prevent the self-signed config
# from acting by mounting /dev/null in its place, then adding your own config
# files in the caddy/snippets directory
# WARNING: if you disable the self-signed certificate, Caddy will attempt to get a
# real certificate for PUBLIC_HOSTNAME from LetsEncrypt.  If this server is
# behind a firewall or NAT, or PUBLIC_HOSTNAME is misconfigured, it will fail
# to get a certificate, and that will count against LetsEncrypt's rate limits.
TLS_SELF_SIGNED_SNIPPET=caddy/self-signed.snippet
#TLS_SELF_SIGNED_SNIPPET=/dev/null

# Caddy will only accept requests on the /admin/ endpoints over https by default.
# This is so that you can password-protect them with HTTP basicauth.
# However, if you've configured your server to only serve http, and something
# upstream is providing SSL, you can change this to allow access to the
# admin endpoints.
# ADMIN_ENDPOINT_PROTOCOL=http

# Monitoring env variables
#
export PROMETHEUS_VERSION=v2.49.1
export NODE_EXPORTER_VERSION=v1.7.0
export CADVISOR_VERSION=v0.47.2
export GRAFANA_VERSION=10.3.3
export LOKI_VERSION=2.9.4
export PROMTAIL_VERSION=2.9.4
export HIVED_PME_VERSION=49a7312d
export BLACKBOX_VERSION=v0.24.0
export DATA_SOURCE="postgresql://postgres@haf:5432/postgres?sslmode=disable"
