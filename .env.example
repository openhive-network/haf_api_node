# The name the ZFS storage pool for HAF to use
ZPOOL="haf-pool"
# The name of the dataset on $ZPOOL where HAF will store its data
# HAF won't read/write anything outside of $ZPOOL/$TOP_LEVEL_DATASET,
# so you can have, e.g., multiple HAF installations on the same 
# pool by changing TOP_LEVEL_DATASET
TOP_LEVEL_DATASET="haf-datadir"

# these defaults usually don't need changing
ZPOOL_MOUNT_POINT="/${ZPOOL}"
TOP_LEVEL_DATASET_MOUNTPOINT="${ZPOOL_MOUNT_POINT}/${TOP_LEVEL_DATASET}"

# COMPOSE_PROFILES are the list of HAF services you want to control when
# you run `docker compose up` etc.  It's a comma-separated list of profiles
# taken from:
# - core: the minimal HAF system of a database and hived
# - admin: useful tools for administrating HAF: pgadmin, pghero
# - apps: core HAF apps: hivemind, hafah, hafbe (balance-tracker is a subapp)
# - servers: services for routing/caching API calls: haproxy, jussi (JSON caching), varnish (REST caching)
COMPOSE_PROFILES="core,admin,hivemind,hafah,servers"

# Global settings
HAF_IMAGE=registry.gitlab.syncad.com/hive/haf/instance:1.27.5rc3
HAF_DATA_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}"
HAF_LOG_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/logs"
# If you need to massive sync HAF (i.e. you are not using a ZFS snapshot),
# then you can sync faster by temporarily using an in-memory shared_memory.bin.
# To do this, comment out the line below and uncomment the one after, and
# mount an appropriate tmpfs filesystem there.
# After the sync has finished, do `docker compose down` then move the shared_memory.bin
# file to the blockchain directory, edit this file to restore original values, and
# `docker compose up -d` to restart HAF.
HAF_SHM_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/blockchain"
#HAF_SHM_DIRECTORY="/mnt/haf_shared_mem"

# The docker compose project name, gets prefixed onto each container name
PROJECT_NAME=haf-world
# The docker network name, if you run two HAF instances on the same server,
# give them different network names to keep them isolated.  Otherwise
# unimportant.
NETWORK_NAME=haf

# List of arguments for the HAF service
#ARGUMENTS="--dump-snapshot=20230821"
#ARGUMENTS="--skip-hived"
#ARGUMENTS="--replay-blockchain"
ARGUMENTS=""
#ARGUMENTS="--replay-blockchain --stop-replay-at-block 21000000 --exit-before-sync"

# The default setup will run the recommended version of balance tracker, 
# you can run a custom version by un-commenting and modifying the
# values below
#BALANCE_TRACKER_REGISTRY=registry.gitlab.syncad.com/hive/balance_tracker
#BALANCE_TRACKER_SETUP_VERSION=haf_api_node

# The default setup will run the recommended version of HAfAH, 
# you can run a custom version by un-commenting and modifying the
# values below
# HAFAH_REGISTRY=registry.gitlab.syncad.com/hive/hafah/setup
HAFAH_VERSION=haf_api_node

# The default setup will run the recommended version of Hivemind using the values
# below.  You can override them here to run a custom version of Hivemind
# HIVEMIND_INSTANCE_IMAGE=registry.gitlab.syncad.com/hive/hivemind/instance
HIVEMIND_INSTANCE_VERSION=1.27.5rc2

# The default setup will run the recommended version of HAF block explorer, 
# you can run a custom version by un-commenting and modifying the
# values below
# HAF_BLOCK_EXPLORER_REGISTRY=registry.gitlab.syncad.com/hive/haf_block_explorer
# HAF_BLOCK_EXPLORER_VERSION=haf_api_node

# For running a full stack:
# if you need to run a custom image (for example, to use ACME DNS challenges), specify it here
#CADDY_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/caddy
#CADDY_IMAGE=caddy:2.7.4-alpine-with-cloudflare

# The hostname you'll be running this server on
PUBLIC_HOSTNAME=your.hostname.com

# By default, we're configured to use a self-signed SSL certificate (by including the
# file below, which tells Caddy to generate a self-signed certificate).  To obtain a real
# certificate from LetsEncrypt or otherwise, you can prevent the self-signed config
# from acting by mounting /dev/null in its place, then adding your own config
# files in the caddy/snippets directory
# WARNING: if you disable the self-signed certificate, Caddy will attempt to get a
# real certificate for PUBLIC_HOSTNAME from LetsEncrypt.  If this server is 
# behind a firewall or NAT, or PUBLIC_HOSTNAME is misconfigured, it will fail
# to get a certificate, and that will count against LetsEncrypt's rate limits.
TLS_SELF_SIGNED_SNIPPET=caddy/self-signed.snippet
#TLS_SELF_SIGNED_SNIPPET=/dev/null

