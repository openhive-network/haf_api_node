# The name the ZFS storage pool for HAF to use
ZPOOL="haf-pool"
# The name of the dataset on $ZPOOL where HAF will store its data
# HAF won't read/write anything outside of $ZPOOL/$TOP_LEVEL_DATASET,
# so you can have, e.g., multiple HAF installations on the same 
# pool by changing TOP_LEVEL_DATASET
TOP_LEVEL_DATASET="haf-datadir"

# these defaults usually don't need changing
ZPOOL_MOUNT_POINT="/${ZPOOL}"
TOP_LEVEL_DATASET_MOUNTPOINT="${ZPOOL_MOUNT_POINT}/${TOP_LEVEL_DATASET}"

# COMPOSE_PROFILES are the list of HAF services you want to control when
# you run `docker compose up` etc.  It's a comma-separated list of profiles
# taken from:
# - core: the minimal HAF system of a database and hived
# - admin: useful tools for administrating HAF: pgadmin, pghero
# - apps: core HAF apps: hivemind, hafah, hafbe (balance-tracker is a subapp)
# - servers: services for routing/caching API calls: haproxy, jussi (JSON caching), varnish (REST caching)
COMPOSE_PROFILES="core,admin,hafah,servers"

# Global settings
HAF_IMAGE=registry.gitlab.syncad.com/hive/haf/instance:v1.27.5rc4
HAF_DATA_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}"
HAF_LOG_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/logs"
# If you need to massive sync HAF (i.e. you are not using a ZFS snapshot),
# then you can sync faster by temporarily using an in-memory shared_memory.bin.
# To do this, comment out the line below and uncomment the one after, and
# mount an appropriate tmpfs filesystem there.
# After the sync has finished, do `docker compose down` then move the shared_memory.bin
# file to the blockchain directory, edit this file to restore original values, and
# `docker compose up -d` to restart HAF.
HAF_SHM_DIRECTORY="${TOP_LEVEL_DATASET_MOUNTPOINT}/blockchain"
#HAF_SHM_DIRECTORY="/mnt/haf_shared_mem"

# The docker compose project name, gets prefixed onto each container name
PROJECT_NAME=haf-world
# The docker network name, if you run two HAF instances on the same server,
# give them different network names to keep them isolated.  Otherwise
# unimportant.
NETWORK_NAME=haf

# List of arguments for the HAF service
#ARGUMENTS="--dump-snapshot=20230821"
#ARGUMENTS="--skip-hived"
#ARGUMENTS="--replay-blockchain"
ARGUMENTS=""
#ARGUMENTS="--replay-blockchain --stop-replay-at-block 21000000 --exit-before-sync"
#

# The default setup will run the recommended version of HAfAH, 
# you can run a custom version by un-commenting and modifying the
# values below
# HAFAH_REGISTRY=registry.gitlab.syncad.com/hive/hafah/setup
HAFAH_VERSION=v1.27.5rc4

# The default setup will run the recommended version of Hivemind using the values
# below.  You can override them here to run a custom version of Hivemind
# HIVEMIND_INSTANCE_IMAGE=registry.gitlab.syncad.com/hive/hivemind/instance
HIVEMIND_INSTANCE_VERSION=v1.27.5rc4 #hivemind version not yet published, wait a couple of days

# The default setup will run the recommended version of balance tracker, 
# you can run a custom version by un-commenting and modifying the
# values below
#BALANCE_TRACKER_REGISTRY=registry.gitlab.syncad.com/hive/balance_tracker
BALANCE_TRACKER_VERSION=v1.27.5rc4

# The default setup will run the recommended version of HAF block explorer, 
# you can run a custom version by un-commenting and modifying the
# values below
# HAF_BLOCK_EXPLORER_REGISTRY=registry.gitlab.syncad.com/hive/haf_block_explorer
HAF_BLOCK_EXPLORER_VERSION=v1.27.5rc4

# The default setup uses "Jussi" as the API reverse proxy & cache for the old JSON-RPC-style 
# calls.  There is an alternate reverse proxy, "Drone", that you can choose to use instead:
# https://hive.blog/hive-139531/@deathwing/announcing-drone-or-leveling-up-hive-api-nodes-and-user-experience
# To replace Jussi with Drone, uncomment the next line:
# JSONRPC_API_SERVER_NAME=drone

# The default setup will run the recommended version of Jussi
# you can run a custom version by un-commenting and modifying the
# values below
# JUSSI_REGISTRY=registry.gitlab.syncad.com/hive/jussi
# JUSSI_VERSION=latest
# JUSSI_REDIS_MAX_MEMORY=8G

# If you have chosen to run Drone instead of Jussi, it will run the
# this version by default.  You can run a custom version by un-commenting
# and modifying the values below
# DRONE_REGISTRY=registry.gitlab.syncad.com/hive/haf_api_node/drone
# DRONE_VERSION=latest

# In the default configuration, synchronous broadcast_transaction calls are not handled by
# your local stack, but instead are sent to a dedicated hived instance on api.hive.blog.
# (asynchronous broadcast_transaction calls and all other hived calls are always handled by
# your local instance of hived).
# Synchronous calls can easily tie up your hived node and cause performance problems for
# all hived API calls.  For that reason, synchronous broadcast calls are deprecated.  On 
# public API servers, we typically run a separate hived instance for synchronous calls,
# so if they cause performance problems, it only impacts other users making synchronous
# calls.
# To avoid forcing every haf_api_node operator to run a second hived server, the default
# config forwards these disruptive calls to a public server dedicated to the purpose.
# If you want to handle these calls using your local hived node, or you want to forward
# these calls to a different server, override these variables:
# the values below will cause synchronous broadcasts to be handled by your own hived
# SYNC_BROADCAST_BACKEND_SERVER=haf
# SYNC_BROADCAST_BACKEND_PORT=8091
# SYNC_BROADCAST_BACKEND_SSL=no-ssl

# For running a full stack:
# if you need to run a custom image (for example, to use ACME DNS challenges), specify it here
#CADDY_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/caddy
#CADDY_IMAGE=caddy:2.7.4-alpine-with-cloudflare

# The hostname you'll be running this server on.  If you want the server to be reachable
# on multiple hostnames, separate them with a comma
PUBLIC_HOSTNAME="your.hostname.com"

# By default, we're configured to use a self-signed SSL certificate (by including the
# file below, which tells Caddy to generate a self-signed certificate).  To obtain a real
# certificate from LetsEncrypt or otherwise, you can prevent the self-signed config
# from acting by mounting /dev/null in its place, then adding your own config
# files in the caddy/snippets directory
# WARNING: if you disable the self-signed certificate, Caddy will attempt to get a
# real certificate for PUBLIC_HOSTNAME from LetsEncrypt.  If this server is 
# behind a firewall or NAT, or PUBLIC_HOSTNAME is misconfigured, it will fail
# to get a certificate, and that will count against LetsEncrypt's rate limits.
TLS_SELF_SIGNED_SNIPPET=caddy/self-signed.snippet
#TLS_SELF_SIGNED_SNIPPET=/dev/null
